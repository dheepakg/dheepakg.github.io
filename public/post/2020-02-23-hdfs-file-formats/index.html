<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Comparison of various HDFS file formats - Dheepak's blog</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Comparison of various HDFS file formats"><meta property="og:description" content="I&rsquo;ve been working with Big data since 2017. As I&rsquo;m from data warehousing background, it was easier for me to understand what&rsquo;s what, and build an analogy between DWH & big data frameworks. However, the various file formats used in HDFS always caught me off guard.
In DWH, I never considered how the files are stored in DB, it&rsquo;s managed by the database, maybe DBA might know how its done at the backend, as a DB developer it never bothered me."><meta property="og:type" content="article"><meta property="og:url" content="/post/2020-02-23-hdfs-file-formats/"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-02-23T13:47:17+05:30"><meta property="article:modified_time" content="2020-02-23T13:47:17+05:30"><meta itemprop=name content="Comparison of various HDFS file formats"><meta itemprop=description content="I&rsquo;ve been working with Big data since 2017. As I&rsquo;m from data warehousing background, it was easier for me to understand what&rsquo;s what, and build an analogy between DWH & big data frameworks. However, the various file formats used in HDFS always caught me off guard.
In DWH, I never considered how the files are stored in DB, it&rsquo;s managed by the database, maybe DBA might know how its done at the backend, as a DB developer it never bothered me."><meta itemprop=datePublished content="2020-02-23T13:47:17+05:30"><meta itemprop=dateModified content="2020-02-23T13:47:17+05:30"><meta itemprop=wordCount content="428"><meta itemprop=keywords content="avro,,parquet,,csv,,json,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Comparison of various HDFS file formats"><meta name=twitter:description content="I&rsquo;ve been working with Big data since 2017. As I&rsquo;m from data warehousing background, it was easier for me to understand what&rsquo;s what, and build an analogy between DWH & big data frameworks. However, the various file formats used in HDFS always caught me off guard.
In DWH, I never considered how the files are stored in DB, it&rsquo;s managed by the database, maybe DBA might know how its done at the backend, as a DB developer it never bothered me."><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel=stylesheet href=/><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Dheepak's blog" rel=home><div class="logo__item logo__text"><div class=logo__title>Dheepak's blog</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/post/><span class=menu__text>Blog Posts</span></a></li><li class=menu__item><a class=menu__link href=/notes/><span class=menu__text>Notes</span></a></li><li class=menu__item><a class=menu__link href=/recommended/><span class=menu__text>Recommended</span></a></li><li class=menu__item><a class=menu__link href=/til/><span class=menu__text>TIL</span></a></li><li class=menu__item><a class=menu__link href=/about/><span class=menu__text>Who am I</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>Comparison of various HDFS file formats</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>Dheepak Govindaraj</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2020-02-23T13:47:17+05:30>2020-02-23</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/hands-on/ rel=category>Hands-On</a></span></div></div></header><div class="content post__content clearfix"><p>I&rsquo;ve been working with Big data since 2017. As I&rsquo;m from data warehousing background, it was easier for me to understand what&rsquo;s what, and build an analogy between DWH & big data frameworks. However, the various file formats used in HDFS always caught me off guard.</p><p>In DWH, I never considered how the files are stored in DB, it&rsquo;s managed by the database, maybe DBA might know how its done at the backend, as a DB developer it never bothered me. However, in HDFS we have several types of formats to choose from - <em>Avro, ORC & Parquet.</em> The best way to understand something is to spend time with it. So, decided to see how the different files behave for same data.</p><p>To see the performance of the file formats, I decided to load the same data in different formats. Identifying the dataset to work with took lots of time. While looking for the data came across a wonderful article to discover free dataset for data science related purpose in <a href=https://www.dataquest.io/blog/free-datasets-for-projects/>Dataquest&rsquo;s blog</a>. I was looking for a csv file with approximately 1 GB of size. I couldn&rsquo;t finalize on the dataset and then my friend suggested <a href=http://files.grouplens.org/datasets/movielens/>movie lens data</a>. Size wise its not close to my requirement, but 500+ MB is good enough.</p><p>We got the data, next ingredient to the dish would be HDFS cluster - AWS EMR. The biggest fear in choosing AWS paid service is the <a href=https://dev.to/juanmanuelramallo/i-was-billed-for-14k-usd-on-amazon-web-services-17fn>fear of overwhelming billing</a> by the book seller. Hiding the fear, took a smallest possible EMR cluster<a href=#f1>1</a>. With all the queries<a href=#f2>2</a> made ready before spinning up the cluster, I did went ahead with the operation.</p><p>All things set, after bootstrapping, the EMR is ready to be connected. And then, <em>connection time out error</em> embraced me while SSHing from EC2. After resolving the issue<a href=#f3>3</a>, the entire usage of EMR went for 28 mins.</p><p><strong>Observations</strong></p><table><thead><tr><th style=text-align:left>File Type</th><th style=text-align:right>File Size (in MB)</th><th style=text-align:right>Time taken to build table (in secs)</th><th style=text-align:right>Time taken to calculate count() (in secs)</th><th style=text-align:right>Time taken to SELECT sample records<a href=#f4>4</a> (in secs)</th></tr></thead><tbody><tr><td style=text-align:left>CSV</td><td style=text-align:right>508.7</td><td style=text-align:right>20.271</td><td style=text-align:right>17.061</td><td style=text-align:right>0.085</td></tr><tr><td style=text-align:left>Avro</td><td style=text-align:right>489.8</td><td style=text-align:right>54.642</td><td style=text-align:right>25.356</td><td style=text-align:right>0.257</td></tr><tr><td style=text-align:left>ORC</td><td style=text-align:right>129.6</td><td style=text-align:right>109.97</td><td style=text-align:right>4.286</td><td style=text-align:right>0.075</td></tr><tr><td style=text-align:left>Parquet</td><td style=text-align:right>307.7</td><td style=text-align:right>58.873</td><td style=text-align:right>17.709</td><td style=text-align:right>0.091</td></tr></tbody></table><p><strong>Footnotes</strong>
1. 1 node of Master & Core is selected with <em>Spot</em> instead of <em>On Demand</em> instance. Below are the cluster details. <a href=#a1>↩</a></p><table><thead><tr><th>Master</th><th style=text-align:center>Core</th><th style=text-align:right>Spot Pricing</th></tr></thead><tbody><tr><td>m5.xlarge</td><td style=text-align:center>m5.xlarge</td><td style=text-align:right>$0.064 per Hr</td></tr></tbody></table><p>2. Queries are available for reference in <a href=https://github.com/dheepakg/file-formats/tree/master/DDLs>Git repo</a> - <a href=#a2>↩</a></p><p>3. In the security group of EMR, inbound SSH connections were blocked. On allowing SSH, the connection is established. <a href=#a3>↩</a></p><p>4. Sample data for SELECT is limited to 11 records. <a href=#a4>↩</a></p></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/avro/ rel=tag>avro,</a></li><li class=tags__item><a class="tags__link btn" href=/tags/parquet/ rel=tag>parquet,</a></li><li class=tags__item><a class="tags__link btn" href=/tags/csv/ rel=tag>csv,</a></li><li class=tags__item><a class="tags__link btn" href=/tags/json/ rel=tag>json</a></li></ul></div></footer></article></main><div class="authorbox clearfix"><div class=authorbox__header><span class=authorbox__name>About Dheepak Govindaraj</span></div></div><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/2020-01-05-the-last-decade/ rel=prev><span class=pager__subtitle>«&#8201;Previous</span><p class=pager__title>The 2010s - Part 1</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/post/2020-04-04-setting-pytest/ rel=next><span class=pager__subtitle>Next&#8201;»</span><p class=pager__title>Setting pytest on zsh shell</p></a></div></nav></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Dheepak Govindaraj.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script><script src=/></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script></body></html>